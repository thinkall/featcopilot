{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FeatCopilot Demo: LLM-Powered Auto Feature Engineering\n",
    "\n",
    "This notebook demonstrates the key capabilities of **FeatCopilot**, a next-generation feature engineering framework:\n",
    "\n",
    "1. **Tabular Feature Engineering** - Automated feature generation with mathematical transformations\n",
    "2. **LLM-Powered Features** - Semantic understanding and domain-aware feature creation\n",
    "3. **Feature Store Integration** - Save and serve features with Feast\n",
    "4. **AutoML Training** - Train models with FLAML using engineered features\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install featcopilot[full] flaml[automl] matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"\u2713 Libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import FeatCopilot\n",
    "from featcopilot import AutoFeatureEngineer\n",
    "from featcopilot.engines import TabularEngine\n",
    "from featcopilot.selection import FeatureSelector\n",
    "\n",
    "# Try to import LLM components\n",
    "try:\n",
    "    from featcopilot.llm import SemanticEngine\n",
    "    LLM_AVAILABLE = True\n",
    "    print(\"\u2713 FeatCopilot with LLM support\")\n",
    "except ImportError:\n",
    "    LLM_AVAILABLE = False\n",
    "    print(\"\u2713 FeatCopilot loaded (LLM support not installed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LLM CONFIGURATION\n",
    "# ============================================\n",
    "# Choose your LLM model. Options include:\n",
    "#   - 'gpt-4o' (requires OPENAI_API_KEY)\n",
    "#   - 'gpt-4o-mini' (requires OPENAI_API_KEY, cheaper)\n",
    "#   - 'github/gpt-4o' (requires GITHUB_API_KEY)\n",
    "#   - 'azure/gpt-4o' (requires Azure OpenAI setup)\n",
    "#   - 'anthropic/claude-3-5-sonnet-20241022' (requires ANTHROPIC_API_KEY)\n",
    "\n",
    "LLM_MODEL = 'gpt-4o'  # Change this to your preferred model\n",
    "\n",
    "import os\n",
    "if LLM_AVAILABLE:\n",
    "    # Check for API keys\n",
    "    if 'OPENAI_API_KEY' in os.environ:\n",
    "        print(f'✓ Using model: {LLM_MODEL} (OPENAI_API_KEY found)')\n",
    "    elif 'GITHUB_API_KEY' in os.environ:\n",
    "        LLM_MODEL = 'github/gpt-4o'\n",
    "        print(f'✓ Using model: {LLM_MODEL} (GITHUB_API_KEY found)')\n",
    "    else:\n",
    "        print('⚠ No API key found. LLM features will use mock responses.')\n",
    "        print('  Set OPENAI_API_KEY or GITHUB_API_KEY environment variable.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset: Healthcare Diabetes Prediction\n",
    "\n",
    "We'll use a synthetic healthcare dataset where feature engineering provides significant benefits.\n",
    "The target depends on **interactions and ratios** between features that simple models can't easily learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_diabetes_dataset(n_samples=2000, random_state=42):\n",
    "    \"\"\"\n",
    "    Create a synthetic diabetes dataset where feature engineering matters.\n",
    "    The target is based on medical ratios and interactions.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    data = pd.DataFrame({\n",
    "        'patient_id': range(1, n_samples + 1),\n",
    "        'event_timestamp': [datetime.now() - timedelta(days=np.random.randint(0, 365)) \n",
    "                           for _ in range(n_samples)],\n",
    "        # Demographics\n",
    "        'age': np.random.randint(25, 85, n_samples),\n",
    "        'bmi': np.random.normal(28, 6, n_samples).clip(16, 50),\n",
    "        # Blood pressure\n",
    "        'bp_systolic': np.random.normal(130, 20, n_samples).clip(90, 200),\n",
    "        'bp_diastolic': np.random.normal(85, 12, n_samples).clip(60, 120),\n",
    "        # Cholesterol\n",
    "        'cholesterol_total': np.random.normal(220, 45, n_samples).clip(120, 350),\n",
    "        'cholesterol_hdl': np.random.normal(50, 15, n_samples).clip(25, 100),\n",
    "        'cholesterol_ldl': np.random.normal(130, 35, n_samples).clip(60, 250),\n",
    "        # Glucose markers\n",
    "        'glucose_fasting': np.random.normal(110, 35, n_samples).clip(70, 250),\n",
    "        'hba1c': np.random.normal(6.0, 1.5, n_samples).clip(4, 14),\n",
    "        # Lifestyle\n",
    "        'smoking_years': np.random.exponential(8, n_samples).clip(0, 50),\n",
    "        'exercise_hours_weekly': np.random.exponential(3, n_samples).clip(0, 20),\n",
    "        'alcohol_drinks_weekly': np.random.exponential(4, n_samples).clip(0, 30),\n",
    "    })\n",
    "    \n",
    "    # Create target based on INTERACTIONS and RATIOS (not linear terms)\n",
    "    # This is what makes feature engineering valuable\n",
    "    glucose_x_hba1c = data['glucose_fasting'] * data['hba1c']\n",
    "    bmi_x_age = data['bmi'] * data['age']\n",
    "    chol_ratio = data['cholesterol_total'] / (data['cholesterol_hdl'] + 1)\n",
    "    bp_product = data['bp_systolic'] * data['bp_diastolic']\n",
    "    lifestyle_risk = data['smoking_years'] / (data['exercise_hours_weekly'] + 1)\n",
    "    \n",
    "    risk_score = (\n",
    "        -8.0\n",
    "        + 0.006 * glucose_x_hba1c\n",
    "        + 0.0008 * bmi_x_age\n",
    "        + 0.08 * chol_ratio\n",
    "        + 0.0003 * bp_product\n",
    "        + 0.05 * lifestyle_risk\n",
    "    )\n",
    "    \n",
    "    # Add noise\n",
    "    risk_score += np.random.normal(0, 0.5, n_samples)\n",
    "    \n",
    "    # Convert to probability and binary outcome\n",
    "    prob = 1 / (1 + np.exp(-risk_score))\n",
    "    data['diabetes'] = (np.random.random(n_samples) < prob).astype(int)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Create dataset\n",
    "data = create_diabetes_dataset(2000)\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Target distribution:\\n{data['diabetes'].value_counts()}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dataset\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "\n",
    "# Target distribution\n",
    "ax = axes[0, 0]\n",
    "data['diabetes'].value_counts().plot(kind='bar', ax=ax, color=['#2ecc71', '#e74c3c'])\n",
    "ax.set_title('Target Distribution', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Diabetes')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xticklabels(['No (0)', 'Yes (1)'], rotation=0)\n",
    "\n",
    "# Age distribution by target\n",
    "ax = axes[0, 1]\n",
    "for label, color in [(0, '#2ecc71'), (1, '#e74c3c')]:\n",
    "    data[data['diabetes'] == label]['age'].hist(ax=ax, alpha=0.6, label=f'Diabetes={label}', color=color, bins=20)\n",
    "ax.set_title('Age Distribution by Target', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "# BMI vs Glucose\n",
    "ax = axes[0, 2]\n",
    "scatter = ax.scatter(data['bmi'], data['glucose_fasting'], c=data['diabetes'], \n",
    "                     cmap='RdYlGn_r', alpha=0.5, s=20)\n",
    "ax.set_xlabel('BMI')\n",
    "ax.set_ylabel('Fasting Glucose')\n",
    "ax.set_title('BMI vs Glucose (colored by target)', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Correlation with target\n",
    "ax = axes[1, 0]\n",
    "feature_cols = ['age', 'bmi', 'bp_systolic', 'cholesterol_total', 'glucose_fasting', 'hba1c']\n",
    "correlations = data[feature_cols + ['diabetes']].corr()['diabetes'].drop('diabetes').sort_values()\n",
    "correlations.plot(kind='barh', ax=ax, color='steelblue')\n",
    "ax.set_title('Feature Correlations with Target', fontsize=12, fontweight='bold')\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# HbA1c distribution\n",
    "ax = axes[1, 1]\n",
    "for label, color in [(0, '#2ecc71'), (1, '#e74c3c')]:\n",
    "    data[data['diabetes'] == label]['hba1c'].hist(ax=ax, alpha=0.6, label=f'Diabetes={label}', color=color, bins=20)\n",
    "ax.set_title('HbA1c Distribution by Target', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "# Feature importance hint: show that ratios matter\n",
    "ax = axes[1, 2]\n",
    "data['chol_ratio_temp'] = data['cholesterol_total'] / (data['cholesterol_hdl'] + 1)\n",
    "for label, color in [(0, '#2ecc71'), (1, '#e74c3c')]:\n",
    "    data[data['diabetes'] == label]['chol_ratio_temp'].hist(ax=ax, alpha=0.6, label=f'Diabetes={label}', color=color, bins=20)\n",
    "ax.set_title('Cholesterol Ratio (Total/HDL) - Engineered Feature', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "data.drop('chol_ratio_temp', axis=1, inplace=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dataset_exploration.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\n\ud83d\udca1 Notice: Raw correlations are weak, but RATIOS (like cholesterol ratio) show stronger separation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns and column descriptions for LLM\n",
    "feature_cols = [\n",
    "    'age', 'bmi', 'bp_systolic', 'bp_diastolic',\n",
    "    'cholesterol_total', 'cholesterol_hdl', 'cholesterol_ldl',\n",
    "    'glucose_fasting', 'hba1c',\n",
    "    'smoking_years', 'exercise_hours_weekly', 'alcohol_drinks_weekly'\n",
    "]\n",
    "\n",
    "# Column descriptions help the LLM understand the data\n",
    "column_descriptions = {\n",
    "    'age': 'Patient age in years',\n",
    "    'bmi': 'Body Mass Index (kg/m\u00b2)',\n",
    "    'bp_systolic': 'Systolic blood pressure in mmHg',\n",
    "    'bp_diastolic': 'Diastolic blood pressure in mmHg',\n",
    "    'cholesterol_total': 'Total cholesterol in mg/dL',\n",
    "    'cholesterol_hdl': 'HDL (good) cholesterol in mg/dL',\n",
    "    'cholesterol_ldl': 'LDL (bad) cholesterol in mg/dL',\n",
    "    'glucose_fasting': 'Fasting blood glucose in mg/dL',\n",
    "    'hba1c': 'Hemoglobin A1c percentage (3-month glucose average)',\n",
    "    'smoking_years': 'Number of years patient has smoked',\n",
    "    'exercise_hours_weekly': 'Average hours of exercise per week',\n",
    "    'alcohol_drinks_weekly': 'Average alcoholic drinks per week',\n",
    "}\n",
    "\n",
    "# Split data\n",
    "X = data[feature_cols].copy()\n",
    "y = data['diabetes']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Model (No Feature Engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline model\n",
    "baseline_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "baseline_pred = baseline_model.predict(X_test)\n",
    "baseline_prob = baseline_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "baseline_accuracy = accuracy_score(y_test, baseline_pred)\n",
    "baseline_auc = roc_auc_score(y_test, baseline_prob)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"BASELINE MODEL (No Feature Engineering)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy: {baseline_accuracy:.4f}\")\n",
    "print(f\"ROC-AUC:  {baseline_auc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, baseline_pred, target_names=['No Diabetes', 'Diabetes']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering with FeatCopilot\n",
    "\n",
    "### 5.1 Tabular Engine (Fast Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Tabular Engine - generates polynomial features, interactions, and transformations\n",
    "tabular_engineer = AutoFeatureEngineer(\n",
    "    engines=['tabular'],\n",
    "    max_features=50,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "X_train_tabular = tabular_engineer.fit_transform(X_train, y_train)\n",
    "X_test_tabular = tabular_engineer.transform(X_test)\n",
    "\n",
    "# Align columns\n",
    "common_cols = [c for c in X_train_tabular.columns if c in X_test_tabular.columns]\n",
    "X_train_tabular = X_train_tabular[common_cols].fillna(0)\n",
    "X_test_tabular = X_test_tabular[common_cols].fillna(0)\n",
    "\n",
    "print(f\"\\nOriginal features: {X_train.shape[1]}\")\n",
    "print(f\"Tabular features: {X_train_tabular.shape[1]}\")\n",
    "print(f\"\\nNew features generated: {X_train_tabular.shape[1] - X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some generated features\n",
    "new_features = [c for c in X_train_tabular.columns if c not in feature_cols][:15]\n",
    "print(\"Sample generated features:\")\n",
    "for i, feat in enumerate(new_features, 1):\n",
    "    print(f\"  {i}. {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 LLM Engine (Semantic Understanding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# LLM Engine - uses semantic understanding to generate domain-aware features\n",
    "LLM_SUCCESS = False\n",
    "if LLM_AVAILABLE:\n",
    "    try:\n",
    "        llm_engineer = AutoFeatureEngineer(\n",
    "            engines=['tabular', 'llm'],\n",
    "            max_features=60,\n",
    "            llm_config={\n",
    "                'model': LLM_MODEL,\n",
    "                'backend': 'litellm',  # Use LiteLLM backend\n",
    "                'max_suggestions': 15,\n",
    "                'domain': 'healthcare',\n",
    "                'validate_features': True,\n",
    "            },\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        X_train_llm = llm_engineer.fit_transform(\n",
    "            X_train, y_train,\n",
    "            column_descriptions=column_descriptions,\n",
    "            task_description=\"Predict Type 2 diabetes risk based on patient health metrics\"\n",
    "        )\n",
    "        X_test_llm = llm_engineer.transform(X_test)\n",
    "\n",
    "        # Align columns\n",
    "        common_cols_llm = [c for c in X_train_llm.columns if c in X_test_llm.columns]\n",
    "        X_train_llm = X_train_llm[common_cols_llm].fillna(0)\n",
    "        X_test_llm = X_test_llm[common_cols_llm].fillna(0)\n",
    "        LLM_SUCCESS = True\n",
    "\n",
    "        print(f\"\\nOriginal features: {X_train.shape[1]}\")\n",
    "        print(f\"LLM-enhanced features: {X_train_llm.shape[1]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ LLM engine failed: {e}\")\n",
    "        print(\"Falling back to tabular features.\")\n",
    "        LLM_AVAILABLE = False\n",
    "\n",
    "if not LLM_AVAILABLE or not LLM_SUCCESS:\n",
    "    # Use tabular features as LLM features when LLM not available\n",
    "    X_train_llm = X_train_tabular.copy()\n",
    "    X_test_llm = X_test_tabular.copy()\n",
    "    llm_engineer = tabular_engineer\n",
    "    print(\"Using tabular features (LLM not available or failed).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature explanations from LLM\n",
    "explanations = llm_engineer.explain_features()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE EXPLANATIONS\")\n",
    "print(\"=\"*60)\n",
    "if explanations:\n",
    "    for i, (name, explanation) in enumerate(list(explanations.items())[:8], 1):\n",
    "        print(f\"\\n{i}. {name}\")\n",
    "        print(f\"   {explanation[:150]}...\" if len(explanation) > 150 else f\"   {explanation}\")\n",
    "else:\n",
    "    print(\"No explanations available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get generated feature code (only available with LLM engine)\n",
    "feature_code = llm_engineer.get_feature_code()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATED FEATURE CODE\")\n",
    "print(\"=\"*60)\n",
    "if feature_code:\n",
    "    for i, (name, code) in enumerate(list(feature_code.items())[:5], 1):\n",
    "        print(f\"\\n# {i}. {name}\")\n",
    "        print(f\"{code}\")\n",
    "else:\n",
    "    print(\"Feature code only available with LLM engine.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Feature Engineering Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feature counts\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Feature count comparison\n",
    "ax = axes[0]\n",
    "feature_counts = {\n",
    "    'Original': X_train.shape[1],\n",
    "    'Tabular Engine': X_train_tabular.shape[1],\n",
    "    'LLM Engine': X_train_llm.shape[1]\n",
    "}\n",
    "bars = ax.bar(feature_counts.keys(), feature_counts.values(), color=['#3498db', '#2ecc71', '#9b59b6'])\n",
    "ax.set_ylabel('Number of Features')\n",
    "ax.set_title('Feature Count by Method', fontsize=12, fontweight='bold')\n",
    "for bar, count in zip(bars, feature_counts.values()):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, str(count), \n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Feature importance comparison (using correlation with target)\n",
    "ax = axes[1]\n",
    "# Get top correlations for each method\n",
    "def get_top_correlations(X, y, n=10):\n",
    "    corrs = X.corrwith(y).abs().sort_values(ascending=False)\n",
    "    return corrs.head(n)\n",
    "\n",
    "orig_corr = get_top_correlations(X_train, y_train)\n",
    "tabular_corr = get_top_correlations(X_train_tabular, y_train)\n",
    "llm_corr = get_top_correlations(X_train_llm, y_train)\n",
    "\n",
    "x_pos = np.arange(3)\n",
    "max_corrs = [orig_corr.max(), tabular_corr.max(), llm_corr.max()]\n",
    "bars = ax.bar(x_pos, max_corrs, color=['#3498db', '#2ecc71', '#9b59b6'])\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(['Original', 'Tabular', 'LLM'])\n",
    "ax.set_ylabel('Max Correlation with Target')\n",
    "ax.set_title('Best Feature Correlation', fontsize=12, fontweight='bold')\n",
    "for bar, corr in zip(bars, max_corrs):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, f'{corr:.3f}', \n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Top features from LLM engine\n",
    "ax = axes[2]\n",
    "top_llm_features = llm_corr.head(8)\n",
    "top_llm_features.plot(kind='barh', ax=ax, color='#9b59b6')\n",
    "ax.set_xlabel('Correlation with Target')\n",
    "ax.set_title('Top LLM-Enhanced Features', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_engineering_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Models with Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models with different feature sets\n",
    "results = {}\n",
    "\n",
    "datasets = {\n",
    "    'Original': (X_train, X_test),\n",
    "    'Tabular Engine': (X_train_tabular, X_test_tabular),\n",
    "    'LLM Engine': (X_train_llm, X_test_llm),\n",
    "}\n",
    "\n",
    "for name, (X_tr, X_te) in datasets.items():\n",
    "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    model.fit(X_tr, y_train)\n",
    "    \n",
    "    pred = model.predict(X_te)\n",
    "    prob = model.predict_proba(X_te)[:, 1]\n",
    "    \n",
    "    results[name] = {\n",
    "        'accuracy': accuracy_score(y_test, pred),\n",
    "        'roc_auc': roc_auc_score(y_test, prob),\n",
    "        'n_features': X_tr.shape[1]\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df['improvement_auc'] = ((results_df['roc_auc'] / results_df.loc['Original', 'roc_auc']) - 1) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL PERFORMANCE COMPARISON (Logistic Regression)\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# ROC-AUC comparison\n",
    "ax = axes[0]\n",
    "colors = ['#3498db', '#2ecc71', '#9b59b6']\n",
    "bars = ax.bar(results_df.index, results_df['roc_auc'], color=colors)\n",
    "ax.set_ylabel('ROC-AUC Score')\n",
    "ax.set_title('Model Performance by Feature Set', fontsize=12, fontweight='bold')\n",
    "ax.set_ylim([0.5, 1.0])\n",
    "for bar, auc in zip(bars, results_df['roc_auc']):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, f'{auc:.4f}', \n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Improvement percentage\n",
    "ax = axes[1]\n",
    "improvements = results_df['improvement_auc']\n",
    "colors_imp = ['#95a5a6' if x <= 0 else '#27ae60' for x in improvements]\n",
    "bars = ax.bar(results_df.index, improvements, color=colors_imp)\n",
    "ax.set_ylabel('Improvement (%)')\n",
    "ax.set_title('ROC-AUC Improvement vs Baseline', fontsize=12, fontweight='bold')\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "for bar, imp in zip(bars, improvements):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3, f'{imp:+.2f}%', \n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_performance_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. AutoML Training with FLAML\n",
    "\n",
    "Let's use FLAML to find the best model with our engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from flaml import AutoML\n",
    "    FLAML_AVAILABLE = True\n",
    "    print(\"FLAML is available!\")\n",
    "except ImportError:\n",
    "    FLAML_AVAILABLE = False\n",
    "    print(\"FLAML not installed. Install with: pip install flaml[automl]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAML_AVAILABLE:\n",
    "    flaml_results = {}\n",
    "    \n",
    "    for name, (X_tr, X_te) in datasets.items():\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training FLAML AutoML with {name} features...\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        automl = AutoML()\n",
    "        automl.fit(\n",
    "            X_tr, y_train,\n",
    "            task='classification',\n",
    "            metric='roc_auc',\n",
    "            time_budget=60,  # 60 seconds per dataset\n",
    "            verbose=0,\n",
    "        )\n",
    "        \n",
    "        pred = automl.predict(X_te)\n",
    "        prob = automl.predict_proba(X_te)[:, 1]\n",
    "        \n",
    "        flaml_results[name] = {\n",
    "            'accuracy': accuracy_score(y_test, pred),\n",
    "            'roc_auc': roc_auc_score(y_test, prob),\n",
    "            'best_model': automl.best_estimator,\n",
    "            'n_features': X_tr.shape[1]\n",
    "        }\n",
    "        \n",
    "        print(f\"  Best model: {automl.best_estimator}\")\n",
    "        print(f\"  ROC-AUC: {flaml_results[name]['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAML_AVAILABLE:\n",
    "    # Display FLAML results\n",
    "    flaml_df = pd.DataFrame(flaml_results).T\n",
    "    flaml_df['improvement_auc'] = ((flaml_df['roc_auc'] / flaml_df.loc['Original', 'roc_auc']) - 1) * 100\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FLAML AutoML RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    print(flaml_df[['accuracy', 'roc_auc', 'improvement_auc', 'best_model']].to_string())\n",
    "    \n",
    "    # Visualize FLAML results\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    x = np.arange(len(datasets))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, results_df['roc_auc'], width, label='Logistic Regression', color='#3498db')\n",
    "    bars2 = ax.bar(x + width/2, flaml_df['roc_auc'], width, label='FLAML AutoML', color='#e74c3c')\n",
    "    \n",
    "    ax.set_ylabel('ROC-AUC Score')\n",
    "    ax.set_title('Logistic Regression vs FLAML AutoML', fontsize=12, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(datasets.keys())\n",
    "    ax.legend()\n",
    "    ax.set_ylim([0.5, 1.0])\n",
    "    \n",
    "    # Add value labels\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.annotate(f'{height:.3f}',\n",
    "                       xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                       xytext=(0, 3), textcoords=\"offset points\",\n",
    "                       ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('flaml_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Store Integration with Feast\n",
    "\n",
    "Save engineered features to Feast for reuse and serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from featcopilot.stores import FeastFeatureStore\n",
    "    import feast  # Also check if feast is actually installed\n",
    "    FEAST_AVAILABLE = True\n",
    "    print(\"✓ Feast integration is available!\")\n",
    "except ImportError:\n",
    "    FEAST_AVAILABLE = False\n",
    "    print(\"⚠ Feast not installed. Install with: pip install featcopilot[feast]\")\n",
    "    print(\"  Skipping feature store integration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FEAST_AVAILABLE:\n",
    "    # Prepare data with entity columns\n",
    "    X_train_feast = X_train_llm.copy()\n",
    "    X_train_feast['patient_id'] = data.loc[X_train.index, 'patient_id'].values\n",
    "    X_train_feast['event_timestamp'] = data.loc[X_train.index, 'event_timestamp'].values\n",
    "    \n",
    "    print(f\"Features to save: {X_train_feast.shape[1] - 2} (excluding entity columns)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FEAST_AVAILABLE:\n",
    "    # Initialize Feast Feature Store\n",
    "    store = FeastFeatureStore(\n",
    "        repo_path='./demo_feature_repo',\n",
    "        project_name='diabetes_prediction',\n",
    "        entity_columns=['patient_id'],\n",
    "        timestamp_column='event_timestamp',\n",
    "        ttl_days=365,\n",
    "        auto_materialize=True,\n",
    "        tags={'team': 'ml', 'domain': 'healthcare', 'created_by': 'featcopilot'}\n",
    "    )\n",
    "    \n",
    "    store.initialize()\n",
    "    print(\"Feast feature store initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FEAST_AVAILABLE:\n",
    "    # Save features to Feast\n",
    "    store.save_features(\n",
    "        df=X_train_feast,\n",
    "        feature_view_name='diabetes_features',\n",
    "        description='Diabetes prediction features generated by FeatCopilot LLM engine'\n",
    "    )\n",
    "    \n",
    "    print(\"\u2705 Features saved to Feast!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FEAST_AVAILABLE:\n",
    "    # List feature views\n",
    "    views = store.list_feature_views()\n",
    "    print(f\"\\nFeature views in store: {views}\")\n",
    "    \n",
    "    # Get schema\n",
    "    schema = store.get_feature_view_schema('diabetes_features')\n",
    "    print(f\"\\nFeature view schema:\")\n",
    "    print(f\"  - Name: {schema.get('name')}\")\n",
    "    print(f\"  - Entities: {schema.get('entities')}\")\n",
    "    print(f\"  - TTL: {schema.get('ttl')}\")\n",
    "    print(f\"  - Features: {len(schema.get('features', []))} features\")\n",
    "    \n",
    "    # Show first few features\n",
    "    print(f\"\\n  Sample features:\")\n",
    "    for feat in schema.get('features', [])[:5]:\n",
    "        print(f\"    - {feat['name']}: {feat['dtype']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FEAST_AVAILABLE:\n",
    "    # Retrieve features for inference (online store)\n",
    "    sample_patients = {'patient_id': [1, 2, 3, 4, 5]}\n",
    "    \n",
    "    # Get feature names (first 5)\n",
    "    feature_names = [c for c in X_train_feast.columns if c not in ['patient_id', 'event_timestamp']][:5]\n",
    "    \n",
    "    online_features = store.get_online_features(\n",
    "        entity_dict=sample_patients,\n",
    "        feature_names=feature_names,\n",
    "        feature_view_name='diabetes_features'\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ONLINE FEATURE RETRIEVAL (Real-time Inference)\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"\\nRetrieved features for patients {sample_patients['patient_id']}:\")\n",
    "    for key, values in online_features.items():\n",
    "        print(f\"  {key}: {values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FEAST_AVAILABLE:\n",
    "    # Visualize feature store architecture\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Create a simple architecture diagram using matplotlib\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 6)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('FeatCopilot + Feast Architecture', fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Boxes\n",
    "    boxes = [\n",
    "        (1, 4, 2, 1.2, 'Raw Data', '#3498db'),\n",
    "        (4, 4, 2, 1.2, 'FeatCopilot', '#9b59b6'),\n",
    "        (7, 4, 2, 1.2, 'Feast Store', '#e74c3c'),\n",
    "        (4, 1.5, 2, 1.2, 'ML Model', '#2ecc71'),\n",
    "        (7, 1.5, 2, 1.2, 'Predictions', '#f39c12'),\n",
    "    ]\n",
    "    \n",
    "    for x, y, w, h, label, color in boxes:\n",
    "        rect = plt.Rectangle((x, y), w, h, facecolor=color, edgecolor='black', linewidth=2, alpha=0.8)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x + w/2, y + h/2, label, ha='center', va='center', fontsize=11, fontweight='bold', color='white')\n",
    "    \n",
    "    # Arrows\n",
    "    arrows = [\n",
    "        (3, 4.6, 0.8, 0),\n",
    "        (6, 4.6, 0.8, 0),\n",
    "        (5, 4, 0, -1.2),\n",
    "        (8, 4, 0, -1.2),\n",
    "        (6, 2.1, 0.8, 0),\n",
    "    ]\n",
    "    \n",
    "    for x, y, dx, dy in arrows:\n",
    "        ax.annotate('', xy=(x+dx, y+dy), xytext=(x, y),\n",
    "                   arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "    \n",
    "    # Labels\n",
    "    ax.text(5, 5.5, 'Feature Engineering Pipeline', ha='center', fontsize=12, style='italic')\n",
    "    ax.text(5, 0.8, 'Training: Offline Store | Inference: Online Store', ha='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feast_architecture.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FEAST_AVAILABLE:\n",
    "    # Cleanup\n",
    "    store.close()\n",
    "    print(\"Feature store connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary & Key Takeaways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary visualization\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Create summary metrics\n",
    "summary_data = {\n",
    "    'Metric': ['Original Features', 'Tabular Features', 'LLM Features',\n",
    "               'Baseline AUC', 'Tabular AUC', 'LLM AUC'],\n",
    "    'Value': [X_train.shape[1], X_train_tabular.shape[1], X_train_llm.shape[1],\n",
    "              results['Original']['roc_auc'], results['Tabular Engine']['roc_auc'], \n",
    "              results['LLM Engine']['roc_auc']]\n",
    "}\n",
    "\n",
    "# Main title\n",
    "fig.suptitle('FeatCopilot Demo Summary', fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "# Subplot 1: Feature generation\n",
    "ax1 = fig.add_subplot(2, 2, 1)\n",
    "methods = ['Original', 'Tabular', 'LLM']\n",
    "features = [X_train.shape[1], X_train_tabular.shape[1], X_train_llm.shape[1]]\n",
    "colors = ['#3498db', '#2ecc71', '#9b59b6']\n",
    "bars = ax1.bar(methods, features, color=colors)\n",
    "ax1.set_ylabel('Number of Features')\n",
    "ax1.set_title('Feature Generation', fontweight='bold')\n",
    "for bar, f in zip(bars, features):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, str(f), ha='center', fontweight='bold')\n",
    "\n",
    "# Subplot 2: Model performance\n",
    "ax2 = fig.add_subplot(2, 2, 2)\n",
    "aucs = [results['Original']['roc_auc'], results['Tabular Engine']['roc_auc'], results['LLM Engine']['roc_auc']]\n",
    "bars = ax2.bar(methods, aucs, color=colors)\n",
    "ax2.set_ylabel('ROC-AUC')\n",
    "ax2.set_title('Model Performance (Logistic Regression)', fontweight='bold')\n",
    "ax2.set_ylim([0.5, 1.0])\n",
    "for bar, a in zip(bars, aucs):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, f'{a:.4f}', ha='center', fontweight='bold')\n",
    "\n",
    "# Subplot 3: Key capabilities\n",
    "ax3 = fig.add_subplot(2, 2, 3)\n",
    "ax3.axis('off')\n",
    "capabilities = \"\"\"\n",
    "\ud83d\udd27 Multi-Engine Architecture\n",
    "   \u2022 Tabular: Polynomials, interactions, transforms\n",
    "   \u2022 LLM: Semantic, domain-aware features\n",
    "\n",
    "\ud83e\udd16 LLM-Powered Intelligence\n",
    "   \u2022 Understands column meanings\n",
    "   \u2022 Generates domain-specific features\n",
    "   \u2022 Provides human-readable explanations\n",
    "\n",
    "\ud83d\udcca Feature Store Integration\n",
    "   \u2022 Save features to Feast\n",
    "   \u2022 Offline: Historical retrieval\n",
    "   \u2022 Online: Real-time serving\n",
    "\"\"\"\n",
    "ax3.text(0.1, 0.9, capabilities, transform=ax3.transAxes, fontsize=11,\n",
    "         verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.3))\n",
    "ax3.set_title('Key Capabilities', fontweight='bold', pad=20)\n",
    "\n",
    "# Subplot 4: Improvement summary\n",
    "ax4 = fig.add_subplot(2, 2, 4)\n",
    "improvements = [\n",
    "    results_df.loc['Tabular Engine', 'improvement_auc'],\n",
    "    results_df.loc['LLM Engine', 'improvement_auc']\n",
    "]\n",
    "methods_imp = ['Tabular Engine', 'LLM Engine']\n",
    "colors_imp = ['#2ecc71', '#9b59b6']\n",
    "bars = ax4.bar(methods_imp, improvements, color=colors_imp)\n",
    "ax4.set_ylabel('Improvement over Baseline (%)')\n",
    "ax4.set_title('Performance Improvement', fontweight='bold')\n",
    "ax4.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "for bar, imp in zip(bars, improvements):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.2, f'+{imp:.2f}%', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('featcopilot_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\ud83d\udcca FEATCOPILOT DEMO SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\"\"\n",
    "Dataset: Synthetic Diabetes Prediction ({len(data)} samples)\n",
    "\n",
    "Feature Engineering Results:\n",
    "  \u2022 Original features:      {X_train.shape[1]}\n",
    "  \u2022 Tabular engine:         {X_train_tabular.shape[1]} features\n",
    "  \u2022 LLM engine:             {X_train_llm.shape[1]} features\n",
    "\n",
    "Model Performance (Logistic Regression):\n",
    "  \u2022 Baseline ROC-AUC:       {results['Original']['roc_auc']:.4f}\n",
    "  \u2022 Tabular ROC-AUC:        {results['Tabular Engine']['roc_auc']:.4f} ({results_df.loc['Tabular Engine', 'improvement_auc']:+.2f}%)\n",
    "  \u2022 LLM ROC-AUC:            {results['LLM Engine']['roc_auc']:.4f} ({results_df.loc['LLM Engine', 'improvement_auc']:+.2f}%)\n",
    "\n",
    "Key Takeaways:\n",
    "  \u2705 FeatCopilot automatically generates predictive features\n",
    "  \u2705 LLM engine provides semantic understanding and explanations\n",
    "  \u2705 Feature store integration enables production deployment\n",
    "  \u2705 Works seamlessly with AutoML (FLAML)\n",
    "\"\"\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83c\udf89 Conclusion\n",
    "\n",
    "This notebook demonstrated FeatCopilot's key capabilities:\n",
    "\n",
    "1. **Tabular Engine**: Fast feature generation (<1s) with polynomials, interactions, and transforms\n",
    "2. **LLM Engine**: Semantic understanding for domain-aware feature creation with explanations\n",
    "3. **Feature Store**: Feast integration for feature reuse and serving\n",
    "4. **AutoML**: Seamless integration with FLAML for automated model selection\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try FeatCopilot on your own datasets\n",
    "- Explore different LLM backends (OpenAI, Anthropic, etc.)\n",
    "- Deploy features to production with Feast\n",
    "- Combine with your favorite ML frameworks\n",
    "\n",
    "\ud83d\udcda **Documentation**: [https://thinkall.github.io/featcopilot/](https://thinkall.github.io/featcopilot/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flaml312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
